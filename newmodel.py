# -*- coding: utf-8 -*-
"""newmodel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iyCZ6Ax-oMrQg3OQdkQi5SfV6SCSTfOR
"""

!pip install transformers
!pip install torch
!pip install streamlit
!pip install SpeechRecognition
!pip install gtts

import streamlit as st  # Make sure to import streamlit
import torch
from transformers import MarianMTModel, MarianTokenizer
import speech_recognition as sr
from gtts import gTTS
import os
import datetime

def is_allowed_time():
    current_time = datetime.datetime.now().time()
    if current_time >= datetime.time(18, 0):  # After 6 PM IST
        return True
    else:
        return False

st.title("English to Hindi Audio Translation")

if is_allowed_time():
    audio_file = st.file_uploader("Upload an audio file", type=["wav", "mp3"])
    if audio_file is not None:
        text = recognize_speech_from_file(audio_file)
        st.write(f"Recognized English Text: {text}")
        translated_text = translate_text(text)
        st.write(f"Translated Hindi Text: {translated_text}")
        audio_output = convert_to_audio(translated_text)
        st.audio(audio_output, format='audio/mp3')
else:
    st.write("Translation service is available after 6 PM IST.")

!pip install transformers
!pip install torch
!pip install streamlit
!pip install SpeechRecognition
!pip install gtts
!pip install sacrebleu  # Library for calculating BLEU score

!pip install datasets

import sacrebleu
import torch

def evaluate_bleu_score(model, tokenizer, test_data):
    references = []
    hypotheses = []

    # Ensure the model is on the correct device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    for item in test_data:
        english_text = item["english"]
        expected_hindi = item["hindi"]

        # Tokenize and move inputs to the correct device
        inputs = tokenizer(english_text, return_tensors="pt", padding=True, truncation=True).to(device)
        translated_tokens = model.generate(**inputs)
        translated_hindi = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

        references.append([expected_hindi])
        hypotheses.append(translated_hindi)

    # Calculate BLEU score
    bleu = sacrebleu.corpus_bleu(hypotheses, references)
    return bleu.score

test_data = [
    {"english": "Hello, how are you?", "hindi": "नमस्ते, आप कैसे हैं?"},
    {"english": "What is your name?", "hindi": "आपका नाम क्या है?"},
    {"english": "I love to play football.", "hindi": "मुझे फुटबॉल खेलना बहुत पसंद है।"},
    {"english": "How old are you?", "hindi": "आपकी उम्र क्या है?"},
    {"english": "I am from India.", "hindi": "मैं भारत से हूँ।"},
    {"english": "What do you do?", "hindi": "आप क्या करते हैं?"},
    {"english": "I like to read books.", "hindi": "मुझे किताबें पढ़ना पसंद है।"},
    {"english": "Do you speak English?", "hindi": "क्या आप अंग्रेज़ी बोलते हैं?"},
    {"english": "I have a dog.", "hindi": "मेरे पास एक कुत्ता है।"},
    {"english": "What is your favorite food?", "hindi": "आपका पसंदीदा खाना क्या है?"},
    {"english": "I love to travel.", "hindi": "मुझे यात्रा करना बहुत पसंद है।"},
    {"english": "How much does it cost?", "hindi": "इसकी कीमत क्या है?"},
    {"english": "I am happy to meet you.", "hindi": "आपसे मिलने में मुझे खुशी है।"}

]

# Example: Extending fine-tuning epochs and using more data
training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=10,  # Increase number of epochs
    predict_with_generate=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets,
    eval_dataset=tokenized_datasets,
)

trainer.train()

bleu_score = evaluate_bleu_score(model, tokenizer, test_data)
print(f"Accuracy: {bleu_score:.2f}%")

